{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a994fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c012d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n",
      "None\n",
      "tensor([15., 18.])\n"
     ]
    }
   ],
   "source": [
    "# gradient for scalar\n",
    "x = Variable(torch.Tensor([2,3]), requires_grad=True)  # tensor([2., 3.], requires_grad=True)\n",
    "x1 = x + 3  # tensor([5., 6.], grad_fn=<AddBackward0>)\n",
    "x2 = x1*x1*3 # 同上\n",
    "y = x2.mean()  #tensor(91.5000, grad_fn=<MeanBackward0>)\n",
    "\n",
    "# print(y.data)   # tensor(91.5000)\n",
    "print(x.data)  #tensor([2., 3.])\n",
    "print(x.grad)  # None\n",
    "y.backward()\n",
    "print(x.grad)  #tensor([15., 18.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd0d2a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "==== non scalar output ======\n",
      "input\n",
      "tensor([[2., 3.]])\n",
      "input gradient are\n",
      "tensor([[14.0000, 25.5000]])\n"
     ]
    }
   ],
   "source": [
    "# gradient for vectors\n",
    "m = Variable(torch.Tensor([[2,3]]), requires_grad=True)\n",
    "n = Variable(torch.zeros(1,2))\n",
    "v = Variable(torch.zeros(1,2))\n",
    "n[0,0] = m[0,0]**2\n",
    "n[0,1] = m[0,1]**3\n",
    "v[0,0] = m[0,0]**2 + m[0,1]**2\n",
    "v[0,1] = m[0,0]**3 + m[0,1]**3\n",
    "\n",
    "# n.backward(torch.Tensor([[0,1]]))\n",
    "v.backward(torch.Tensor([[2,0.5]]))  \n",
    "#此处的系数是，因变量（结果）两个结果分别对x1和x2求导完后的系数:\n",
    "# 比如上面 v[k1,k2], 这里求导是对X1：求导（k1/x1）+求导(k2/x1)。然后backward的系数是k1和k2的系数\n",
    "print('*' * 10)\n",
    "print('==== non scalar output ======')\n",
    "print('input')\n",
    "print(m.data)\n",
    "print('input gradient are')\n",
    "print(m.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4990b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) None None\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]]) None <MulBackward0 object at 0x7faa83fec6d0>\n",
      "tensor(40.) None <MeanBackward0 object at 0x7fa9d1af9090>\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]]) None <AddBackward0 object at 0x7faa83fec6d0>\n",
      "tensor([[8., 8.],\n",
      "        [8., 8.]]) None <MulBackward0 object at 0x7fa9d1af9090>\n",
      "tensor([[40., 40.],\n",
      "        [40., 40.]]) None <MulBackward0 object at 0x7faa83fec6d0>\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) None None\n",
      "tensor(2.) tensor(28.) None\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]]) None <MulBackward0 object at 0x7faa83fec6d0>\n",
      "tensor(40.) None <MeanBackward0 object at 0x7fa9cb2d0d90>\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]]) None <AddBackward0 object at 0x7faa83fec6d0>\n",
      "tensor([[8., 8.],\n",
      "        [8., 8.]]) None <MulBackward0 object at 0x7fa9cb2d0d90>\n",
      "tensor([[40., 40.],\n",
      "        [40., 40.]]) None <MulBackward0 object at 0x7faa83fec6d0>\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]]) None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  if sys.path[0] == '':\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  del sys.path[0]\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  \n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "  app.launch_new_instance()\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n"
     ]
    }
   ],
   "source": [
    "input = torch.ones([2, 2], requires_grad=False)\n",
    "w1 = torch.tensor(2.0, requires_grad=True)\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "l1 = input * w1\n",
    "l2 = l1 + w2\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "loss = l4.mean()\n",
    "\n",
    "# 反向传播前\n",
    "print(w1.data, w1.grad, w1.grad_fn)  # tensor(2.) None None\n",
    "print(l1.data, l1.grad, l1.grad_fn)   # tensor([[2., 2.],[2., 2.]]) None <MulBackward0 object at 0x000001EBE79E6AC8>\n",
    "print(loss.data, loss.grad, loss.grad_fn)  # tensor(40.) None <MeanBackward0 object at 0x000001EBE79D8208>\n",
    "print(l2.data, l2.grad, l2.grad_fn)  #tensor([[5., 5.],[5., 5.]]) None <AddBackward0 object at 0x7fca426aba50>\n",
    "print(l3.data, l3.grad, l3.grad_fn) # 8 8 8 8  None <MulBackward0 object at 0x7fca42b83bd0>\n",
    "print(l4.data, l4.grad, l4.grad_fn)   # 40 40 40 40  None <MulBackward0 object at 0x7fca426aba50>\n",
    "print(input.data, input.grad, input.grad_fn)   # tensor([[1., 1.],[1., 1.]]) None None\n",
    "\n",
    "# 反向传播后（求导后）\n",
    "loss.backward()\n",
    "print(w1.data, w1.grad, w1.grad_fn)  # tensor(2.) tensor(28.) None\n",
    "print(l1.data, l1.grad, l1.grad_fn)   # tensor([[2., 2.],[2., 2.]]) None <MulBackward0 object at 0x000001EBE79E6AC8>\n",
    "print(loss.data, loss.grad, loss.grad_fn)  # tensor(40.) None <MeanBackward0 object at 0x000001EBE79D8208>\n",
    "print(l2.data, l2.grad, l2.grad_fn)  #tensor([[5., 5.],[5., 5.]]) None <AddBackward0 object at 0x7fca426aba50>\n",
    "print(l3.data, l3.grad, l3.grad_fn) # 8 8 8 8  None <MulBackward0 object at 0x7fca42b83bd0>\n",
    "print(l4.data, l4.grad, l4.grad_fn)   # 40 40 40 40  None <MulBackward0 object at 0x7fca426aba50>\n",
    "print(input.data, input.grad, input.grad_fn)   # tensor([[1., 1.],[1., 1.]]) None None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aca7d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(w1.is_leaf)   # True\n",
    "print(input.is_leaf)   # True\n",
    "print(l1.is_leaf)   # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16468b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
