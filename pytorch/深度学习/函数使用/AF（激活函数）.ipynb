{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf60379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数是什么，有什么用？不用激活函数可不可以？\n",
    "# 不可以。激活函数的主要作用是提供网络的非线性建模能力。如果没有激活函数，那么该网络仅能够表达线性映射，此时即便有再多的隐藏层，其整个网络跟单层神经网络也是等价的。\n",
    "# 因此也可以认为，只有加入了激活函数之后，深度神经网络才具备了分层的非线性映射学习能力。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54722e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb474261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.3176, -0.1396])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(2)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e94ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Relu = max(0, x)\n",
    "relu = nn.ReLU(inplace=True)  # inplace=False(default)\n",
    "print(relu(data))\n",
    "# output = relu(data)\n",
    "# print(output)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c890f77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8656, 0.5539])\n",
      "tensor([0.7038, 0.6350])\n"
     ]
    }
   ],
   "source": [
    "# sigmoid = sigma(x) = 1/(1+exp(-x))\n",
    "data = torch.rand(2)\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(data)\n",
    "print(sigmoid(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "978add7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3129, -1.1915, -0.0125])\n",
      "tensor([ 0.3031, -0.8311, -0.0125])\n"
     ]
    }
   ],
   "source": [
    "# tanh = tanh(x) = (exp(x)-exp(-x)) / (exp(x)+exp(-x))\n",
    "data = torch.randn(3)\n",
    "print(data)\n",
    "tanh = nn.Tanh()\n",
    "print(tanh(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf31c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.,  0.,  1.])\n",
      "tensor([-0.0100,  0.0000,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "# LeakyReLU = x if x>=0; else negative_slope*x\n",
    "# LeakyReLU = max(0, x) + negative_slope * min(0, x)\n",
    "data = torch.tensor([-1., 0., 1.])\n",
    "print(data)\n",
    "leaky_relu = nn.LeakyReLU()   # 注意要用浮点数字\n",
    "print(leaky_relu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e713609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.,  0.,  2.])\n",
      "tensor([-0.2000,  0.0000,  2.0000], grad_fn=<PreluBackward>)\n"
     ]
    }
   ],
   "source": [
    "# PReLU = max(0, x) + a*min(0, x)\n",
    "data = torch.tensor([-2., 0., 2.])\n",
    "print(data)\n",
    "prelu = nn.PReLU(init=0.1)\n",
    "print(prelu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a079cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.,  0.,  2.])\n",
      "tensor([-0.2156,  0.0000,  2.0000])\n"
     ]
    }
   ],
   "source": [
    "# RReLU = x if x>=0; else ax\n",
    "data = torch.tensor([-2., 0., 2.])\n",
    "print(data)\n",
    "rrelu = nn.RReLU(lower=0.1, upper=0.2)  \n",
    "# 在lower和upper之间随机取的，若想取一个值，lower=upper即可a\n",
    "print(rrelu(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1793aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.,  0.,  2.])\n",
      "tensor([-2.,  0.,  2.])\n"
     ]
    }
   ],
   "source": [
    "# ELU = max(0, x) + min(0, alpha*(exp(x)-1))\n",
    "data = torch.tensor([-2., 0., 2.])\n",
    "print(data)\n",
    "elu = nn.ELU()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a340c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
