{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fac45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part of code is the reinforcement learning brain, which is a brain of the agent.\n",
    "All decisions are made in here.\n",
    "Policy Gradient, Reinforcement Learning.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.95,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        self._build_net()\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        if output_graph:\n",
    "            # $ tensorboard --logdir=logs\n",
    "            # http://0.0.0.0:6006/\n",
    "            # tf.train.SummaryWriter soon be deprecated, use following\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def _build_net(self):\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.tf_obs = tf.placeholder(tf.float32, [None, self.n_features], name=\"observations\")\n",
    "            self.tf_acts = tf.placeholder(tf.int32, [None, ], name=\"actions_num\")\n",
    "            self.tf_vt = tf.placeholder(tf.float32, [None, ], name=\"actions_value\")\n",
    "        # fc1\n",
    "        layer = tf.layers.dense(\n",
    "            inputs=self.tf_obs,\n",
    "            units=10,\n",
    "            activation=tf.nn.tanh,  # tanh activation\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "        # fc2\n",
    "        all_act = tf.layers.dense(\n",
    "            inputs=layer,\n",
    "            units=self.n_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )\n",
    "\n",
    "        self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')  # use softmax to convert to probability\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            # to maximize total reward (log_p * R) is to minimize -(log_p * R), and the tf only have minimize(loss)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_act, labels=self.tf_acts)   # this is negative log of chosen action\n",
    "            # or in this way:\n",
    "            # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "            loss = tf.reduce_mean(neg_log_prob * self.tf_vt)  # reward guided loss\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        prob_weights = self.sess.run(self.all_act_prob, feed_dict={self.tf_obs: observation[np.newaxis, :]})\n",
    "        action = np.random.choice(range(prob_weights.shape[1]), p=prob_weights.ravel())  # select action w.r.t the actions prob\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        # discount and normalize episode reward\n",
    "        discounted_ep_rs_norm = self._discount_and_norm_rewards()\n",
    "\n",
    "        # train on episode\n",
    "        self.sess.run(self.train_op, feed_dict={\n",
    "             self.tf_obs: np.vstack(self.ep_obs),  # shape=[None, n_obs]\n",
    "             self.tf_acts: np.array(self.ep_as),  # shape=[None, ]\n",
    "             self.tf_vt: discounted_ep_rs_norm,  # shape=[None, ]\n",
    "        })\n",
    "\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []    # empty episode data\n",
    "        return discounted_ep_rs_norm\n",
    "\n",
    "    def _discount_and_norm_rewards(self):\n",
    "        # discount episode rewards\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * self.gamma + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        # normalize episode rewards\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        return discounted_ep_rs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
