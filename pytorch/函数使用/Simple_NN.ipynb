{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f942bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de42135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn包可以进行神经网络的构建\n",
    "# nn是在autograd基础上进行模型的定义和微分\n",
    "# nn.module包含着神经网络的层，同时forward（input）方法可以将output进行返回\n",
    "# torcn.nn是专门为神经网络设计的模块化接口. nn构建于autograd之上，可以用来定义和运行神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b27b187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        # nn.Module子类的函数必须在构造函数中执行父类的构造函数\n",
    "        # nn.Module.__init__(self)\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Pytorch基于nn.Module构建的模型中，只支持mini-batch的Variable输入方式。比如，只有一张输入图片，也需要变成NxCxHxW的\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        \n",
    "        # 仿射层/全连接层，y = Wx + b\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 卷积 -> 激活 -> 池化 \n",
    "        x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x=F.max_pool2d(F.relu(self.conv2(x)),2)\n",
    "        \n",
    "        # reshape，‘-1’表示自适应\n",
    "        x=x.view(-1,self.num_flat_features(x))\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self,x):\n",
    "        size=x.size()[1:]\n",
    "        num_features=1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "            \n",
    "# 只需要定义一个forward函数，backward会自动生成，可以在forward函数中使用所有tensor操作\n",
    "# 参数的模型由net.parameters()返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb549da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n神经网络的输出结果是这样的\\nNet (\\n(conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\\n(conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\\n(fc1): Linear (400 -> 120)\\n(fc2): Linear (120 -> 84)\\n(fc3): Linear (84 -> 10)\\n)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "神经网络的输出结果是这样的\n",
    "Net (\n",
    "(conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "(conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
    "(fc1): Linear (400 -> 120)\n",
    "(fc2): Linear (120 -> 84)\n",
    "(fc3): Linear (84 -> 10)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66807f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# 网络的可学习参数通过net.parameters()返回，net.named_parameters可同时返回可学习的参数及名称。\n",
    "net = Net()\n",
    "params = list(net.parameters())\n",
    "print(len(params))   # 10\n",
    "\n",
    "for param in params:\n",
    "    print(param.size())\n",
    "    # print(param)\n",
    "print(params[0].size()) # conv1's .weight: torch.Size([6, 1, 5, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2646161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.5332,  1.0975,  0.1023,  ..., -0.2556,  0.5774,  1.1490],\n",
      "          [ 0.1875,  0.9849,  1.3684,  ..., -1.0755,  0.9035, -0.3509],\n",
      "          [ 0.9821, -0.9314,  0.3585,  ..., -0.6360,  0.4030, -0.7901],\n",
      "          ...,\n",
      "          [-1.0882,  0.0960, -0.3340,  ..., -0.1762, -1.5024, -3.0941],\n",
      "          [-0.0826,  0.8419,  0.0853,  ..., -0.9719,  1.3274,  0.6328],\n",
      "          [ 1.0482,  0.2472, -0.5855,  ..., -0.9211, -2.1074,  1.2623]]]])\n",
      "torch.Size([1, 10])\n",
      "tensor([[ 0.1292,  0.0208,  0.0956,  0.0975, -0.0057, -0.0272, -0.1178,  0.0040,\n",
      "          0.0899, -0.0236]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# forward函数的输入和输出都是Tensor。\n",
    "# 需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。\n",
    "# 但如果只想输入一个样本，则用 input.unsqueeze(0)将batch_size设为１。\n",
    "# 例如 nn.Conv2d 输入必须是4维的，形如nSamples × nChannels × Height × Width. 可将nSample设为1，即1 × nChannels × Height × Width\n",
    "\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net.forward(input)  # net.forward\n",
    "\n",
    "print(input)\n",
    "print(out.size())\n",
    "print(out)\n",
    " \n",
    "net.zero_grad()   # 所有参数的梯度清零\n",
    "out.backward(torch.randn(1,10))   # 反向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "733e2219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.4187, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/anaconda3/envs/pytorch_gpu/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninput -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \\n      -> view -> linear -> relu -> linear -> relu -> linear \\n      -> MSELoss\\n      -> loss\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 损失函数: nn实现了神经网络中大多数的损失函数，例如nn.MSELoss用来计算均方误差，nn.CrossEntropyLoss用来计算交叉熵损失。\n",
    "# 已经定义了一个神经网络，处理了输入以及实现了反馈\n",
    "# 仍未定义代价函数，计算代价，更新网络权重\n",
    "# 代价函数：接收（输出，目标）对作为输入，计算出输出和目标之间差距作为代价函数输出\n",
    "output = net(input)\n",
    "target = Variable(torch.range(1,10))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)  # loss是一个scalar\n",
    "print(loss)\n",
    "\n",
    "\"\"\"\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n",
    "      -> view -> linear -> relu -> linear -> relu -> linear \n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f5fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "反向传播之前 conv1.bias的梯度\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "反向传播之后 conv1.bias的梯度\n",
      "tensor([-0.1493,  0.1278, -0.0439, -0.0774,  0.0175,  0.0143])\n"
     ]
    }
   ],
   "source": [
    "# 当调用loss.backward()时，该图会动态生成并自动微分，也即会自动计算图中参数(Parameter)的导数。\n",
    "# 运行.backward，观察调用之前和调用之后的grad\n",
    "net.zero_grad() # 把net中所有可学习参数的梯度清零\n",
    "print('反向传播之前 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('反向传播之后 conv1.bias的梯度')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b540e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器：在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：\n",
    "weight = weight - learning_rate * gradient\n",
    "\n",
    "# 手动实现\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)# inplace 减法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b89b3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动实现：torch.optim中实现了深度学习中绝大多数的优化方法，例如RMSProp、Adam、SGD等，更便于使用，因此大多数时候并不需要手动写上述代码。\n",
    "import torch.optim as optim\n",
    "#新建一个优化器，指定要调整的参数和学习率\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# 在训练过程中\n",
    "# 先梯度清零(与net.zero_grad()效果一样)\n",
    "optimizer.zero_grad() \n",
    "\n",
    "# 计算损失\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "#反向传播\n",
    "loss.backward()\n",
    "\n",
    "#更新参数\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7c00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
