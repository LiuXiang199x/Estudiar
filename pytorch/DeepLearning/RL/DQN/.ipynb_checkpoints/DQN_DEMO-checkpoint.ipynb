{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "384c2fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b520d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters for DQN\n",
    "GAMMA = 0.9  # discount factor for target Q\n",
    "INITIAL_EPSILON = 0.5  # starting value of epsilon\n",
    "FINAL_EPSILON = 0.01  # final value of epsilon\n",
    "REPLAY_SIZE = 10000  # experience replay buffer size\n",
    "EXPLORE = 10000\n",
    "BATCH_SIZE = 32  # size of minibatch\n",
    "LR = 0.0001  # learning rate\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False  # 非确定性算法\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 20)\n",
    "        self.fc2 = nn.Linear(20, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            nn.init.normal_(m.weight.data, 0, 0.1)\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9daa714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    # dqn Agent\n",
    "    def __init__(self, env):  # 初始化\n",
    "        # 状态空间和动作空间的维度\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        # init experience replay\n",
    "        self.replay_buffer = deque()  # 经验回放池\n",
    "\n",
    "        # init network parameters\n",
    "        self.network = QNetwork(state_dim=self.state_dim, action_dim=self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "        self.epsilon = INITIAL_EPSILON  # epsilon值是随机不断变小的\n",
    "\n",
    "    # store_transition\n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        one_hot_action = np.zeros(self.action_dim)   # 用独热向量保存动作\n",
    "        one_hot_action[action] = 1    # 选中的动作为1，其余为0\n",
    "        # 将该Transition保存到经验回放池\n",
    "        self.replay_buffer.append((state, one_hot_action, reward, next_state, done))\n",
    "        if len(self.replay_buffer) > REPLAY_SIZE:   # 如果经验回放池溢出，扔掉左边的数据\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "        if len(self.replay_buffer) > BATCH_SIZE:    # 只有经验回放池大于mini_batch数了才能采样训练\n",
    "            self.train_Q_network()\n",
    "\n",
    "    def train_Q_network(self):\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Step 1: obtain random minibatch from replay memory\n",
    "        minibatch = random.sample(self.replay_buffer, BATCH_SIZE)   # 32的list\n",
    "        state_batch = torch.FloatTensor([data[0] for data in minibatch]).to(device)   # 32*4\n",
    "        action_batch = torch.LongTensor([data[1] for data in minibatch]).to(device)   # 32*2\n",
    "        reward_batch = torch.FloatTensor([data[2] for data in minibatch]).to(device)  # 32*1\n",
    "        next_state_batch = torch.FloatTensor([data[3] for data in minibatch]).to(device)  # 32*4\n",
    "        done = torch.FloatTensor([data[4] for data in minibatch]).to(device)\n",
    "\n",
    "        done = done.unsqueeze(1)\n",
    "        reward_batch = reward_batch.unsqueeze(1)\n",
    "        # q_val = self.network.forward(state_batch)  # 32*2\n",
    "        action_index = action_batch.argmax(dim=1).unsqueeze(1)  # 32*1\n",
    "        eval_q = self.network.forward(state_batch).gather(1, action_index)   # 32*1\n",
    "\n",
    "        # Step 2: calculate y\n",
    "        Q_value_batch = self.network.forward(next_state_batch)\n",
    "        next_action_batch = torch.unsqueeze(torch.max(Q_value_batch, 1)[1], 1)\n",
    "        next_q = self.network.forward(next_state_batch).gather(1, next_action_batch)\n",
    "\n",
    "        y_batch = reward_batch + GAMMA * next_q * (1 - done)\n",
    "        # y_batch = torch.tensor(y_batch).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # 更新网络\n",
    "        loss = self.loss_func(eval_q, y_batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def egreedy_action(self, state):    # epsilon-greedy策略\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state).to(device), 0)  # 给state加一个batch_size的维度，此时batch_size为1\n",
    "        Q_value = self.network.forward(state.to(device))\n",
    "        # Q_value = self.Q_value.eval(feed_dict={\n",
    "        #   self.state_input: [state]\n",
    "        #   })[0]\n",
    "        if random.random() <= self.epsilon:\n",
    "          self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "          return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "          self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / 10000\n",
    "          return torch.max(Q_value, 1)[1].data.to('cpu').numpy()[0]\n",
    "\n",
    "    def action(self, state):   # 贪婪选择\n",
    "        state = torch.unsqueeze(torch.FloatTensor(state).to(device), 0)  # 给state加一个batch_size的维度，此时batch_size为1\n",
    "        Q_value = self.network.forward(state.to(device))\n",
    "        return torch.max(Q_value, 1)[1].data.to('cpu').numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a305078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: 9.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8446/15669061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The total time is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8446/15669061.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m               \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m               \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# direct action for test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m               \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \"\"\"\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Hyper Parameters\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 3000  # Episode limitation\n",
    "STEP = 300  # Step limitation in an episode\n",
    "TEST = 10  # The number of experiment test every 100 episode\n",
    "\n",
    "def main():\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = env.unwrapped  # 打开限制操作\n",
    "    agent = DQN(env)\n",
    "\n",
    "    for episode in range(EPISODE):\n",
    "        # initialize task\n",
    "        state = env.reset()\n",
    "        # Train\n",
    "        for step in range(STEP):\n",
    "          action = agent.egreedy_action(state)  # e-greedy action for train\n",
    "          next_state, reward, done, _ = env.step(action)\n",
    "          # Define reward for agent\n",
    "          reward = -1 if done else 0.1\n",
    "          agent.perceive(state, action, reward, next_state, done)\n",
    "          state = next_state\n",
    "          if done:\n",
    "            break\n",
    "        # Test every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "          total_reward = 0\n",
    "          for i in range(TEST):\n",
    "            state = env.reset()\n",
    "            for j in range(STEP):\n",
    "              env.render()\n",
    "              action = agent.action(state)  # direct action for test\n",
    "              state, reward, done, _ = env.step(action)\n",
    "              total_reward += reward\n",
    "              if done:\n",
    "                break\n",
    "          ave_reward = total_reward/TEST\n",
    "          print ('episode: ', episode, 'Evaluation Average Reward:', ave_reward)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.time()\n",
    "    main()\n",
    "    time_end = time.time()\n",
    "    print('The total time is ', time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d5e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class DeepQNetwork:\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        ...\n",
    "    def choose_action(self, observation):\n",
    "        ...\n",
    "    def _replace_target_params(self):\n",
    "        ...\n",
    "    def learn(self):\n",
    "        ...\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
