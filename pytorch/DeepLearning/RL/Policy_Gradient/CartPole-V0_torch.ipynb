{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33571f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch as t\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ca471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction = 'none')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "dicount_factor = 0.99\n",
    "eplison = 0.1 #增加动作选择的随机性\n",
    "lr = 0.02\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(1)     # reproducible, general Policy gradient has high variance\n",
    "env = env.unwrapped\n",
    "batch_size = 1\n",
    "epochs = 1000\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        #下面定义两个全连接层就可以了\n",
    "        self.linear1 = nn.Linear(4, 10)\n",
    "        nn.init.normal_(self.linear1.weight, 0, 0.3)\n",
    "        nn.init.constant_(self.linear1.bias, 0.1)\n",
    "        self.linear2 = nn.Linear(10, 2)\n",
    "        nn.init.normal_(self.linear2.weight, 0, 0.3)\n",
    "        nn.init.constant_(self.linear2.bias, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = t.from_numpy(x).float()\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = F.tanh(out)\n",
    "                \n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        # 这个输出主要是用来使用概率来挑选动作\n",
    "        prob = F.softmax(out, dim = 1) \n",
    "        \n",
    "        # prob动作的概率分布softmax处理后的，处理前的动作分布\n",
    "        return prob, out\n",
    "\n",
    "def choose_action(prob):\n",
    "\n",
    "    action = np.random.choice(a = 2, p = prob[0].detach().numpy())\n",
    "    \n",
    "    return action\n",
    "          \n",
    "def get_one_batch(agent):\n",
    "                \n",
    "    reward_an_episode = []\n",
    "    observation_an_episode = []\n",
    "    action_an_episode = []\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        env.render()\n",
    "        observation = np.expand_dims(observation, axis = 0)\n",
    "        prob, log_prob = agent(observation)\n",
    "        observation_an_episode.append(observation)\n",
    "        action = choose_action(prob)\n",
    "        action_an_episode.append(action)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        reward_an_episode.append(reward)\n",
    "            \n",
    "    # concatenate 数组拼接\n",
    "    return action_an_episode, np.concatenate(observation_an_episode, axis = 0), reward_an_episode\n",
    "    \n",
    "\n",
    "def learn():\n",
    "    \n",
    "    #定义一个网络实例\n",
    "    agent = Agent()\n",
    "    \n",
    "    train_loss = []\n",
    "    train_reward = []\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        #定义一个优化器\n",
    "        optim = t.optim.Adam(agent.parameters(),lr = lr)\n",
    "        batch_data = get_one_batch(agent)\n",
    " \n",
    "        #下面开始计算损失函数，要注意，这里的损失函数是有agent所获得奖励的来的\n",
    "        #先计算奖励的累计。这里train就是一个epoch\n",
    "        agent.train()\n",
    "        \n",
    "        # 1.先根据网络结果选动作，进行蒙特卡洛采样以获得一个完整的episode，\n",
    "        # 2.即具有序列时间步的(s,a,r)；利用(s,a)作为训练数据，\n",
    "        # 3.训练三层的神经网络，神经网络的最后一层是softmax函数。\n",
    "        # 4.损失函数设置为交叉熵损失（s状态下真实动作a和网络预测动作a的差距）\n",
    "        # 和价值函数（在一个episode中，每个时间步状态的值函数）的乘积。\n",
    "        \n",
    "        # done=True就三结束了train然后开始learn\n",
    "        actions = t.tensor(batch_data[0])\n",
    "        observations = batch_data[1]\n",
    "        rewards = batch_data[2]\n",
    "        train_reward.append(sum(rewards))\n",
    "        \n",
    "        acc_reward = []\n",
    "        # 现在得到的reward是每个action的reward。\n",
    "        # 我们要站在第一个action时候对后续所有reward进行衰减，也就是乘上gama\n",
    "        for i in range(len(rewards)):\n",
    "            acc_r = 0\n",
    "            for j in range(i, len(rewards)):\n",
    "                acc_r += dicount_factor ** (j-i) * rewards[j]\n",
    "            acc_reward.append(acc_r)\n",
    "            \n",
    "        # 处理所有reward使其满足标准正太分布\n",
    "        acc_reward = t.tensor(acc_reward)\n",
    "        acc_reward -= acc_reward.mean()  \n",
    "        acc_reward /= acc_reward.std()\n",
    "        \n",
    "        # 最大化奖励(log_p * R)就是最小化-(log_p * R)\n",
    "        # and the tf only have minimize(loss)\n",
    "        # neg_log_prob = tf.reduce_sum(-tf.log(self.all_act_prob)*tf.one_hot(self.tf_acts, self.n_actions), axis=1)\n",
    "        # reward guided loss\n",
    "        prob, logits = agent(observations)        \n",
    "        log_prob = criterion(logits, actions)       \n",
    "        log_reward = log_prob * acc_reward\n",
    "        \n",
    "        loss = log_reward.mean()\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(train_reward)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    learn()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ea7e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'method'>\n",
      "<class 'generator'>\n",
      "[Parameter containing:\n",
      "tensor([[ 0.6110, -0.1681, -0.1035, -0.0988],\n",
      "        [-0.1482,  0.1996, -0.1390, -0.2136],\n",
      "        [ 0.0256,  0.1145, -0.4795,  0.2283],\n",
      "        [-0.3503,  0.0462, -0.2296, -0.3252],\n",
      "        [-0.0177,  0.1209,  0.2520,  0.1926],\n",
      "        [-0.0536,  0.0241, -0.0305, -0.1277],\n",
      "        [-0.0820, -0.1940,  0.0118, -0.0192],\n",
      "        [-0.3222, -0.0980,  0.2215,  0.0351],\n",
      "        [ 0.4188,  0.1760,  0.0363, -0.1564],\n",
      "        [-0.0116, -0.3528, -0.0363, -0.1773]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "        0.1000], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.3352,  0.1983, -0.3473,  0.0352,  0.3200,  0.1734, -0.2385,  0.6449,\n",
      "         -0.3146,  0.1317],\n",
      "        [ 0.2413,  0.2805,  0.5875, -0.1457,  0.0265,  0.0962,  0.0038, -0.1957,\n",
      "          0.0339,  0.0359]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1000, 0.1000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class Agent2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Agent2, self).__init__()\n",
    "        \n",
    "        #下面定义两个全连接层就可以了\n",
    "        self.linear1 = nn.Linear(4, 10)\n",
    "        nn.init.normal_(self.linear1.weight, 0, 0.3)\n",
    "        nn.init.constant_(self.linear1.bias, 0.1)\n",
    "        self.linear2 = nn.Linear(10, 2)\n",
    "        nn.init.normal_(self.linear2.weight, 0, 0.3)\n",
    "        nn.init.constant_(self.linear2.bias, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = t.from_numpy(x).float()\n",
    "        \n",
    "        out = self.linear1(out)\n",
    "        out = F.tanh(out)\n",
    "                \n",
    "        out = self.linear2(out)\n",
    "        \n",
    "        # 这个输出主要是用来使用概率来挑选动作\n",
    "        prob = F.softmax(out, dim = 1) \n",
    "        \n",
    "        return prob, out\n",
    "av = Agent2()\n",
    "print(type(av.parameters))\n",
    "print(type(av.parameters()))\n",
    "print(list(av.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8e6f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor([-1.5000, -0.5000,  0.5000,  1.5000])\n",
      "tensor(1.2910)\n",
      "tensor([-1.1619, -0.3873,  0.3873,  1.1619])\n",
      "tensor(0.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1.,2.,3.,4.])\n",
    "print(a.mean())\n",
    "print(a.std())\n",
    "b = a - a.mean()\n",
    "print(b)\n",
    "print(b.std())\n",
    "b = b / b.std()\n",
    "print(b)\n",
    "print(b.mean())\n",
    "print(b.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4b723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
