{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c0bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                q_table:\n",
      "    left  right\n",
      "0   0.0    0.0\n",
      "1   0.0    0.0\n",
      "2   0.0    0.0\n",
      "3   0.0    0.0\n",
      "4   0.0    0.1\n",
      "5   0.0    0.0\n",
      "                                q_table:\n",
      "    left  right\n",
      "0   0.0  0.000\n",
      "1   0.0  0.000\n",
      "2   0.0  0.000\n",
      "3   0.0  0.009\n",
      "4   0.0  0.190\n",
      "5   0.0  0.000\n",
      "                                q_table:\n",
      "    left    right\n",
      "0   0.0  0.00000\n",
      "1   0.0  0.00000\n",
      "2   0.0  0.00081\n",
      "3   0.0  0.02520\n",
      "4   0.0  0.27100\n",
      "5   0.0  0.00000\n",
      "                                q_table:\n",
      "    left     right\n",
      "0   0.0  0.000000\n",
      "1   0.0  0.000073\n",
      "2   0.0  0.002997\n",
      "3   0.0  0.047070\n",
      "4   0.0  0.343900\n",
      "5   0.0  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.000007\n",
      "1  0.00000  0.000572\n",
      "2  0.00003  0.006934\n",
      "3  0.00000  0.073314\n",
      "4  0.00000  0.409510\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.000057\n",
      "1  0.00000  0.001138\n",
      "2  0.00003  0.012839\n",
      "3  0.00000  0.102839\n",
      "4  0.00000  0.468559\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.000154\n",
      "1  0.00000  0.002180\n",
      "2  0.00003  0.020810\n",
      "3  0.00000  0.134725\n",
      "4  0.00000  0.521703\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.000335\n",
      "1  0.00000  0.003835\n",
      "2  0.00003  0.030854\n",
      "3  0.00000  0.168206\n",
      "4  0.00000  0.569533\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.000647\n",
      "1  0.00000  0.006228\n",
      "2  0.00003  0.042907\n",
      "3  0.00000  0.202643\n",
      "4  0.00000  0.612580\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.001142\n",
      "1  0.00000  0.009467\n",
      "2  0.00003  0.056855\n",
      "3  0.00000  0.237511\n",
      "4  0.00000  0.651322\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "       left     right\n",
      "0  0.00000  0.001880\n",
      "1  0.00000  0.013637\n",
      "2  0.00003  0.072545\n",
      "3  0.00000  0.272379\n",
      "4  0.00000  0.686189\n",
      "5  0.00000  0.000000\n",
      "                                q_table:\n",
      "        left     right\n",
      "0  0.000000  0.002920\n",
      "1  0.000000  0.018803\n",
      "2  0.000030  0.089805\n",
      "3  0.000000  0.337965\n",
      "4  0.027621  0.717570\n",
      "5  0.000000  0.000000\n",
      "                                q_table:\n",
      "        left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n",
      "\n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000000  0.004320\n",
      "1  0.000000  0.025005\n",
      "2  0.000030  0.111241\n",
      "3  0.000000  0.368750\n",
      "4  0.027621  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A simple example for Reinforcement Learning using table lookup Q-learning method.\n",
    "An agent \"o\" is on the left of a 1 dimensional world, the treasure is on the rightmost location.\n",
    "Run this program and to see how the agent will improve its strategy of finding the treasure.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)  # reproducible\n",
    "# -o---T\n",
    "# T 就是宝藏的位置, o是探索者的位置\n",
    "\n",
    "N_STATES = 6   # the length of the 1 dimensional world\n",
    "ACTIONS = ['left', 'right']     # available actions\n",
    "EPSILON = 0.9   # greedy police\n",
    "ALPHA = 0.1     # learning rate\n",
    "GAMMA = 0.9    # discount factor：90% 的时间是选择最优策略, 10% 的时间来探索\n",
    "MAX_EPISODES = 13   # maximum episodes\n",
    "FRESH_TIME = 0.01    # fresh time for one move\n",
    "\n",
    "\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),     # q_table initial values\n",
    "        columns=actions,    # actions's name\n",
    "    )\n",
    "    # print(table)    # show table\n",
    "    return table\n",
    "\"\"\"\n",
    "   left  right\n",
    "0   0.0    0.0\n",
    "1   0.0    0.0\n",
    "2   0.0    0.0\n",
    "3   0.0    0.0\n",
    "4   0.0    0.0\n",
    "5   0.0    0.0\n",
    "\"\"\"\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # This is how to choose an action\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):  # act non-greedy or state-action have no value\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "    else:   # act greedy\n",
    "        # replace argmax to idxmax as argmax means a different function in newer version of pandas\n",
    "        action_name = state_actions.idxmax()    \n",
    "    return action_name\n",
    "\n",
    "\"\"\"\n",
    "做出行为后,环境也要给我们的行为一个反馈,反馈出下个state(S_)和在上个state(S)做出action(A)所得到的reward (R). \n",
    "这里定义的规则就是,只有当o移动到了T,探索者才会得到唯一的一个奖励,奖励值R=1,其他情况都没有奖励.\n",
    "\"\"\"\n",
    "def get_env_feedback(S, A):\n",
    "    # This is how agent will interact with the environment\n",
    "    if A == 'right':    # move right\n",
    "        if S == N_STATES - 2:   # terminate\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # move left\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # reach the wall\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # main part of RL loop\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            # take action & get next state and reward\n",
    "            S_, R = get_env_feedback(S, A)  \n",
    "            q_predict = q_table.loc[S, A]\n",
    "            #print(\"q_predict:\", q_predict)\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + GAMMA * q_table.iloc[S_, :].max()   # next state is not terminal\n",
    "                # print(\"q_target:\",q_target)\n",
    "            else:\n",
    "                q_target = R     # next state is terminal\n",
    "                is_terminated = True    # terminate this episode\n",
    "            # update table\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict) \n",
    "            S = S_  # move to next state\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "        print(\"q_table:\\n\", q_table)\n",
    "    return q_table\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97d9b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a  b\n",
      "aa  1  2\n",
      "bb  3  4\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(2, 2)\n",
      "----index, columns, loc------\n",
      "Index(['aa', 'bb'], dtype='object')\n",
      "Index(['a', 'b'], dtype='object')\n",
      "2\n",
      "-----iloc, head, tail------\n",
      "1\n",
      "table2.iloc[0,:]: a    1\n",
      "b    2\n",
      "Name: aa, dtype: int64\n",
      "len(table2.iloc[0,:]): 2\n",
      "    a  b\n",
      "aa  1  2\n",
      "    a  b\n",
      "bb  3  4\n",
      "-----convert rows and colums------\n",
      "   aa  bb\n",
      "a   1   3\n",
      "b   2   4\n",
      "没一个0，就返回True\n",
      "True\n",
      "False\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "False\n",
      "---------\n",
      "0.5076174866719614\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "table2 = pd.DataFrame(np.array([[1,2],[3,4]]), columns=[\"a\", 'b'], \n",
    "                      index=['aa','bb'])\n",
    "print(table2)\n",
    "print(table2.values)\n",
    "print(type(table2))\n",
    "print(table2.shape)\n",
    "print(\"----index, columns, loc------\")\n",
    "print(table2.index)\n",
    "print(table2.columns)\n",
    "print(table2.loc[table2.index[0], table2.columns[1]])\n",
    "print(\"-----iloc, head, tail------\")\n",
    "print(table2.iloc[0,0])\n",
    "print(\"table2.iloc[0,:]:\", table2.iloc[0,:])\n",
    "print(\"len(table2.iloc[0,:]):\",len(table2.iloc[0,:]))\n",
    "print(table2.head(1))   # 前N行\n",
    "print(table2.tail(1))   # 后\n",
    "print(\"-----convert rows and colums------\")\n",
    "print(table2.T)\n",
    "print(\"没一个0，就返回True\")\n",
    "print(np.array([1,2,3]).all())\n",
    "print(np.array([1,2,3]).all()==0)\n",
    "print(np.array([1,0,3]).all())\n",
    "print(np.array([1,0,3]).all()==0)\n",
    "print(np.array([0,0]).all())\n",
    "print(np.array([0,0]).all()==0)\n",
    "print(np.array([]).all())\n",
    "print(np.array([]).all()==0)\n",
    "print(\"---------\")\n",
    "print(np.random.uniform())   # deault: 0-1, 可以自己定义low/high\n",
    "print(np.array([1,2,3]).argmax())  # #取出list中元素最大值所对应的索引\n",
    "print(np.random.choice([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971e101",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
