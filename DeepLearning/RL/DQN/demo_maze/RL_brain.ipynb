{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8f8f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570122bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, 10)\n",
    "        self.fc2 = nn.Linear(10, n_actions)\n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, n_states, n_actions):\n",
    "        print(\"<DQN init>\")\n",
    "        # DQN有两个net:target net和eval net,具有选动作，存经历，学习三个基本功能\n",
    "        self.eval_net, self.target_net = Net(n_states, n_actions), Net(n_states, n_actions)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), lr=0.01)\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        # 使用变量\n",
    "        self.learn_step_counter = 0  # target网络学习计数\n",
    "        self.memory_counter = 0  # 记忆计数\n",
    "        # 2*2(state和next_state,每个x,y坐标确定)+2(action和reward),存储2000个记忆体\n",
    "        self.memory = np.zeros((2000, 2 * 2 + 2))  \n",
    "        self.cost = []  # 记录损失值\n",
    "\n",
    "    def choose_action(self, x, epsilon):\n",
    "        # print(\"<choose_action>\")\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), 0)  # (1,2)\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action_value = self.eval_net.forward(x)\n",
    "            action = torch.max(action_value, 1)[1].data.numpy()[0]\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        # print(\"action=\", action)\n",
    "        return action\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # print(\"<store_transition>\")\n",
    "        transition = np.hstack((state, [action, reward], next_state))\n",
    "        index = self.memory_counter % 200  # 满了就覆盖旧的\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def learn(self):\n",
    "        # print(\"<learn>\")\n",
    "        # target net 更新频率,用于预测，不会及时更新参数\n",
    "        if self.learn_step_counter % 100 == 0:\n",
    "            self.target_net.load_state_dict((self.eval_net.state_dict()))\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # 使用记忆库中批量数据\n",
    "        sample_index = np.random.choice(200, 16)  # 2000个中随机抽取32个作为batch_size\n",
    "        memory = self.memory[sample_index, :]  # 抽取的记忆单元，并逐个提取\n",
    "        state = torch.FloatTensor(memory[:, :2])\n",
    "        action = torch.LongTensor(memory[:, 2:3])\n",
    "        reward = torch.LongTensor(memory[:, 3:4])\n",
    "        next_state = torch.FloatTensor(memory[:, 4:6])\n",
    "\n",
    "        # 计算loss,q_eval:所采取动作的预测value,q_target:所采取动作的实际value\n",
    "        q_eval = self.eval_net(state).gather(1, action) # eval_net->(64,4)->按照action索引提取出q_value\n",
    "        q_next = self.target_net(next_state).detach()\n",
    "        # torch.max->[values=[],indices=[]] max(1)[0]->values=[]\n",
    "        q_target = reward + 0.9 * q_next.max(1)[0].unsqueeze(1) # label\n",
    "        loss = self.loss(q_eval, q_target)\n",
    "        self.cost.append(loss)\n",
    "        # 反向传播更新\n",
    "        self.optimizer.zero_grad()  # 梯度重置\n",
    "        loss.backward()  # 反向求导\n",
    "        self.optimizer.step()  # 更新模型参数\n",
    "\n",
    "    def plot_cost(self):\n",
    "        plt.plot(np.arange(len(self.cost)), self.cost)\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b2b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
