{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b493e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from unet import UNet\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from utils.dataset import BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "720acc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.4.0\n",
      "onnx.__version__: 1.6.0\n",
      "onnxruntime.__version__: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"onnx.__version__:\", onnx.__version__)\n",
    "print(\"onnxruntime.__version__:\", onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2361c1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: None\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnxpath)\n",
    "check = onnx.checker.check_model(onnx_model)\n",
    "print(\"check:\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c700a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "#     bs = torch.tensor(1)\n",
    "#     ch = torch.tensor(4)\n",
    "#     H = torch.tensor(481)\n",
    "#     W = torch.tensor(481)\n",
    "    \n",
    "    tmp_h_0 = torch.zeros(120, 481)\n",
    "    tmp_h_1 = torch.ones(120, 481)\n",
    "    tmp_v_0 = torch.zeros(721, 120)\n",
    "    tmp_v_1 = torch.ones(721, 120)\n",
    "    \n",
    "    ############### first map ######################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h0 = torch.cat((tmp_map[0][0], tmp_h_1), dim=0)\n",
    "    tmp_h0 = torch.cat((tmp_h_1, tmp_h0), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v0 = torch.cat((tmp_h0, tmp_v_1), dim=1)\n",
    "    tmp_v0 = torch.cat((tmp_v_1, tmp_v0), dim=1)\n",
    "    \n",
    "    ################ second map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h1 = torch.cat((tmp_map[0][1], tmp_h_0), dim=0)\n",
    "    tmp_h1 = torch.cat((tmp_h_0, tmp_h1), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v1 = torch.cat((tmp_h1, tmp_v_0), dim=1)\n",
    "    tmp_v1 = torch.cat((tmp_v_0, tmp_v1), dim=1)\n",
    "\n",
    "    ################ Third map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h2 = torch.cat((tmp_map[0][2], tmp_h_0), dim=0)\n",
    "    tmp_h2 = torch.cat((tmp_h_0, tmp_h2), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v2 = torch.cat((tmp_h2, tmp_v_0), dim=1)\n",
    "    tmp_v2 = torch.cat((tmp_v_0, tmp_v2), dim=1)\n",
    "    # print(\"tmp_v:\", tmp_v.size())\n",
    "\n",
    "    ################ Forth map ####################\n",
    "\n",
    "    # tensor_index = torch.linspace(120., 601., 481)\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h3 = torch.cat((tmp_map[0][3], tmp_h_0), dim=0)\n",
    "    tmp_h3 = torch.cat((tmp_h_0, tmp_h3), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v3 = torch.cat((tmp_h3, tmp_v_0), dim=1)\n",
    "    tmp_v3 = torch.cat((tmp_v_0, tmp_v3), dim=1)\n",
    "    \n",
    "    ################################################\n",
    "    map_tmp = torch.cat((torch.unsqueeze(tmp_v0, 0), torch.unsqueeze(tmp_v1, 0), \n",
    "                         torch.unsqueeze(tmp_v2, 0), torch.unsqueeze(tmp_v3, 0)), 0)\n",
    "    map_tmp = torch.unsqueeze(map_tmp, 0)\n",
    "    # print(\"after expanding map size:\", map_tmp.size())\n",
    "\n",
    "    ############ start crap map #####################\n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    \n",
    "    # print(map_tmp)\n",
    "    # print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    index_tensor = torch.linspace(-120, 119, 240)\n",
    "    # index_tensor = torch.tensor([-120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
    "    # index_tensor = index_tensor.long()\n",
    "    index_tensor_x = index_tensor.long() + x_pos.long()\n",
    "    index_tensor_y = index_tensor.long() + y_pos.long()\n",
    "    # print(\"index_tensor_x\", index_tensor_x)\n",
    "\n",
    "    tmp_crap_map_0 = torch.index_select(map_tmp[0][0], 1, index_tensor_y)\n",
    "    tmp_crap_map_0 = torch.index_select(tmp_crap_map_0, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_1 = torch.index_select(map_tmp[0][1], 1, index_tensor_y)\n",
    "    tmp_crap_map_1 = torch.index_select(tmp_crap_map_1, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_2 = torch.index_select(map_tmp[0][2], 1, index_tensor_y)\n",
    "    tmp_crap_map_2 = torch.index_select(tmp_crap_map_2, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_3 = torch.index_select(map_tmp[0][3], 1, index_tensor_y)\n",
    "    tmp_crap_map_3 = torch.index_select(tmp_crap_map_3, 0, index_tensor_x)\n",
    "    \n",
    "    tmp = tmp_crap_map_3\n",
    "    \n",
    "    '''\n",
    "    output = torch.cat((torch.unsqueeze(tmp_crap_map_0, 0), torch.unsqueeze(tmp_crap_map_1, 0),\n",
    "                       torch.unsqueeze(tmp_crap_map_2, 0), torch.unsqueeze(tmp_crap_map_3, 0)), 0)\n",
    "    output = torch.unsqueeze(output, 0)\n",
    "    # print(\"ouput::::\",output.size())\n",
    "    '''\n",
    "    # output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    '''\n",
    "    output[0][0] = tmp_crap_map_0\n",
    "    output[0][1] = tmp_crap_map_1\n",
    "    output[0][2] = tmp_crap_map_2\n",
    "    output[0][3] = tmp_crap_map_3\n",
    "    '''\n",
    "    \n",
    "    tmp_0 = tmp_crap_map_0\n",
    "    tmp_1 = tmp_crap_map_1\n",
    "    tmp_2 = tmp_crap_map_2\n",
    "    tmp_3 = tmp_crap_map_3\n",
    "    tmp_0 = torch.reshape(tmp_0,[1,1,tmp_0.shape[0],tmp_0.shape[1]])\n",
    "    tmp_1 = torch.reshape(tmp_1,[1,1,tmp_1.shape[0],tmp_1.shape[1]])\n",
    "    tmp_2 = torch.reshape(tmp_2,[1,1,tmp_2.shape[0],tmp_2.shape[1]])\n",
    "    tmp_3 = torch.reshape(tmp_3,[1,1,tmp_3.shape[0],tmp_3.shape[1]])\n",
    "    \n",
    "    tmp = torch.cat((tmp_0,tmp_1,tmp_2,tmp_3),1)\n",
    "#     tmp_0 = torch.unsqueeze(tmp_crap_map_0, 0)\n",
    "#     tmp_1 = torch.unsqueeze(tmp_crap_map_1, 0)\n",
    "#     tmp_2 = torch.unsqueeze(tmp_crap_map_2, 0)\n",
    "#     tmp_3 = torch.unsqueeze(tmp_crap_map_3, 0)\n",
    "    \n",
    "#     tmp = torch.cat((tmp_0, tmp_1), 0)\n",
    "#     tmp = torch.cat((tmp, tmp_2), 0)\n",
    "#     tmp = torch.cat((tmp, tmp_3), 0)\n",
    "#     tmp = torch.unsqueeze(tmp, 0)\n",
    "    # print(\"tmp size:\", tmp.size())\n",
    "    \n",
    "    # tmp = torch.index_select(tmp, 0, torch.tensor([0]))\n",
    "    # print(\"tmp size:\", tmp.size())\n",
    "    # output = tmp\n",
    "    # print(\"output:\", tmp.size())\n",
    "    \n",
    "    \n",
    "    return tmp\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = torch.tensor(G)\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            Flatten(),  # (G*G, )\n",
    "            nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "        \n",
    "        h_1 = crop_map(h, self.G)\n",
    "        # print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        #print(\"h111:\", h_1.size())\n",
    "        #print(\"h22:\", h_2.size())\n",
    "    \n",
    "        h_12 = torch.cat((h_1, h_2), dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f80705bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[ 0, 20, 20]])\n",
      "tensor([[20, 20]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Global_Actor(\n",
       "  (actor): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(2, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (13): Flatten()\n",
       "    (14): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "print(torch.nonzero(inputs[0][3]==1))\n",
    "\n",
    "rl_net = Global_Actor(240)\n",
    "rl_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "rl_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31a0e1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 481, 481])\n",
      "pth model output: tensor([[0.4694, 0.4706, 0.4692,  ..., 0.4675, 0.4727, 0.4738]],\n",
      "       grad_fn=<SigmoidBackward>)\n",
      "torch.Size([1, 57600])\n",
      "onnx model output: [array([[0.47428057, 0.4646493 , 0.46358562, ..., 0.4679515 , 0.468348  ,\n",
      "        0.47043318]], dtype=float32)]\n",
      "tor_out:  torch.Size([1, 57600])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-02 11:17:26.765408255 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.0.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767253293 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.0.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767263465 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.1.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767270279 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.1.num_batches_tracked appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767277020 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.1.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767283833 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.1.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767290363 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.1.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767296846 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.10.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767303363 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.10.num_batches_tracked appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767309809 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.10.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767316518 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.10.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767324904 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.10.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767331157 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.12.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767338127 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.12.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767345491 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.3.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767351918 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.3.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767358167 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.4.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767364321 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.4.num_batches_tracked appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767370989 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.4.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767377610 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.4.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767385186 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.4.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767391612 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.6.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767398064 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.6.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767404862 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.7.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767411180 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.7.num_batches_tracked appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767417317 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.7.running_mean appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767423776 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.7.running_var appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767430645 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.7.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767436975 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.9.bias appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.767445027 [W:onnxruntime:, graph.cc:1069 Graph] Initializer actor.9.weight appears in graph inputs and will not be treated as constant value/weight. This may prevent some of the graph optimizations, like const folding. Move it out of graph inputs if there is no need to override it, by either re-generating the model with latest exporter/converter or with the tool onnxruntime/tools/python/remove_initializer_from_input.py.\n",
      "2021-12-02 11:17:26.768965067 [W:onnxruntime:, graph.cc:3093 CleanUnusedInitializers] Removing initializer 'actor.7.num_batches_tracked'. It is not used by any node and should be removed from the model.\n",
      "2021-12-02 11:17:26.768979103 [W:onnxruntime:, graph.cc:3093 CleanUnusedInitializers] Removing initializer 'actor.10.num_batches_tracked'. It is not used by any node and should be removed from the model.\n",
      "2021-12-02 11:17:26.768986319 [W:onnxruntime:, graph.cc:3093 CleanUnusedInitializers] Removing initializer 'actor.4.num_batches_tracked'. It is not used by any node and should be removed from the model.\n",
      "2021-12-02 11:17:26.768992341 [W:onnxruntime:, graph.cc:3093 CleanUnusedInitializers] Removing initializer 'actor.1.num_batches_tracked'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "print(x.size())\n",
    "torch_out = rl_net(x)\n",
    "print(\"pth model output:\", torch_out)\n",
    "print(torch_out.shape)\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnxpath)\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: x.cpu().numpy().astype(np.float32)})\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "\n",
    "print(\"onnx model output:\", ort_outs)\n",
    "print('tor_out: ', torch_out.shape)\n",
    "\n",
    "#np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "#print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10a6ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "#     bs = torch.tensor(1)\n",
    "#     ch = torch.tensor(4)\n",
    "#     H = torch.tensor(481)\n",
    "#     W = torch.tensor(481)\n",
    "    \n",
    "    tmp_h_0 = torch.zeros(120, 481)\n",
    "    tmp_h_1 = torch.ones(120, 481)\n",
    "    tmp_v_0 = torch.zeros(721, 120)\n",
    "    tmp_v_1 = torch.ones(721, 120)\n",
    "    \n",
    "    ############### first map ######################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h0 = torch.cat((tmp_map[0][0], tmp_h_1), dim=0)\n",
    "    tmp_h0 = torch.cat((tmp_h_1, tmp_h0), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v0 = torch.cat((tmp_h0, tmp_v_1), dim=1)\n",
    "    tmp_v0 = torch.cat((tmp_v_1, tmp_v0), dim=1)\n",
    "    \n",
    "    ################ second map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h1 = torch.cat((tmp_map[0][1], tmp_h_0), dim=0)\n",
    "    tmp_h1 = torch.cat((tmp_h_0, tmp_h1), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v1 = torch.cat((tmp_h1, tmp_v_0), dim=1)\n",
    "    tmp_v1 = torch.cat((tmp_v_0, tmp_v1), dim=1)\n",
    "\n",
    "    ################ Third map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h2 = torch.cat((tmp_map[0][2], tmp_h_0), dim=0)\n",
    "    tmp_h2 = torch.cat((tmp_h_0, tmp_h2), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v2 = torch.cat((tmp_h2, tmp_v_0), dim=1)\n",
    "    tmp_v2 = torch.cat((tmp_v_0, tmp_v2), dim=1)\n",
    "    # print(\"tmp_v:\", tmp_v.size())\n",
    "\n",
    "    ################ Forth map ####################\n",
    "\n",
    "    # tensor_index = torch.linspace(120., 601., 481)\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h3 = torch.cat((tmp_map[0][3], tmp_h_0), dim=0)\n",
    "    tmp_h3 = torch.cat((tmp_h_0, tmp_h3), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v3 = torch.cat((tmp_h3, tmp_v_0), dim=1)\n",
    "    tmp_v3 = torch.cat((tmp_v_0, tmp_v3), dim=1)\n",
    "    \n",
    "    ################################################\n",
    "    map_tmp = torch.cat((torch.unsqueeze(tmp_v0, 0), torch.unsqueeze(tmp_v1, 0), \n",
    "                         torch.unsqueeze(tmp_v2, 0), torch.unsqueeze(tmp_v3, 0)), 0)\n",
    "    map_tmp = torch.unsqueeze(map_tmp, 0)\n",
    "    # print(\"after expanding map size:\", map_tmp.size())\n",
    "\n",
    "    ############ start crap map #####################\n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    \n",
    "    # print(map_tmp)\n",
    "    # print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    index_tensor = torch.linspace(-120, 119, 240)\n",
    "    # index_tensor = torch.tensor([-120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
    "    # index_tensor = index_tensor.long()\n",
    "    index_tensor_x = index_tensor.long() + x_pos.long()\n",
    "    index_tensor_y = index_tensor.long() + y_pos.long()\n",
    "    # print(\"index_tensor_x\", index_tensor_x)\n",
    "\n",
    "    tmp_crap_map_0 = torch.index_select(map_tmp[0][0], 1, index_tensor_y)\n",
    "    tmp_crap_map_0 = torch.index_select(tmp_crap_map_0, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_1 = torch.index_select(map_tmp[0][1], 1, index_tensor_y)\n",
    "    tmp_crap_map_1 = torch.index_select(tmp_crap_map_1, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_2 = torch.index_select(map_tmp[0][2], 1, index_tensor_y)\n",
    "    tmp_crap_map_2 = torch.index_select(tmp_crap_map_2, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_3 = torch.index_select(map_tmp[0][3], 1, index_tensor_y)\n",
    "    tmp_crap_map_3 = torch.index_select(tmp_crap_map_3, 0, index_tensor_x)\n",
    "    \n",
    "    tmp = tmp_crap_map_3\n",
    "    \n",
    "    '''\n",
    "    output = torch.cat((torch.unsqueeze(tmp_crap_map_0, 0), torch.unsqueeze(tmp_crap_map_1, 0),\n",
    "                       torch.unsqueeze(tmp_crap_map_2, 0), torch.unsqueeze(tmp_crap_map_3, 0)), 0)\n",
    "    output = torch.unsqueeze(output, 0)\n",
    "    # print(\"ouput::::\",output.size())\n",
    "    '''\n",
    "    # output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    '''\n",
    "    output[0][0] = tmp_crap_map_0\n",
    "    output[0][1] = tmp_crap_map_1\n",
    "    output[0][2] = tmp_crap_map_2\n",
    "    output[0][3] = tmp_crap_map_3\n",
    "    '''\n",
    "    \n",
    "    tmp_0 = tmp_crap_map_0\n",
    "    tmp_1 = tmp_crap_map_1\n",
    "    tmp_2 = tmp_crap_map_2\n",
    "    tmp_3 = tmp_crap_map_3\n",
    "    tmp_0 = torch.reshape(tmp_0,[1,1,tmp_0.shape[0],tmp_0.shape[1]])\n",
    "    tmp_1 = torch.reshape(tmp_1,[1,1,tmp_1.shape[0],tmp_1.shape[1]])\n",
    "    tmp_2 = torch.reshape(tmp_2,[1,1,tmp_2.shape[0],tmp_2.shape[1]])\n",
    "    tmp_3 = torch.reshape(tmp_3,[1,1,tmp_3.shape[0],tmp_3.shape[1]])\n",
    "    \n",
    "    tmp = torch.cat((tmp_0,tmp_1,tmp_2,tmp_3),1)\n",
    "#     tmp_0 = torch.unsqueeze(tmp_crap_map_0, 0)\n",
    "#     tmp_1 = torch.unsqueeze(tmp_crap_map_1, 0)\n",
    "#     tmp_2 = torch.unsqueeze(tmp_crap_map_2, 0)\n",
    "#     tmp_3 = torch.unsqueeze(tmp_crap_map_3, 0)\n",
    "    \n",
    "#     tmp = torch.cat((tmp_0, tmp_1), 0)\n",
    "#     tmp = torch.cat((tmp, tmp_2), 0)\n",
    "#     tmp = torch.cat((tmp, tmp_3), 0)\n",
    "#     tmp = torch.unsqueeze(tmp, 0)\n",
    "    # print(\"tmp size:\", tmp.size())\n",
    "    \n",
    "    # tmp = torch.index_select(tmp, 0, torch.tensor([0]))\n",
    "    # print(\"tmp size:\", tmp.size())\n",
    "    # output = tmp\n",
    "    # print(\"output:\", tmp.size())\n",
    "    \n",
    "    \n",
    "    return tmp\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        # self.G = torch.tensor(G)\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            Flatten(),  # (G*G, )\n",
    "            nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "        \n",
    "        h_1 = crop_map(h, 240)\n",
    "        # print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        #print(\"h111:\", h_1.size())\n",
    "        #print(\"h22:\", h_2.size())\n",
    "    \n",
    "        h_12 = torch.cat((h_1, h_2), dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46e95c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.linspace(-120, 119, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6dd4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
