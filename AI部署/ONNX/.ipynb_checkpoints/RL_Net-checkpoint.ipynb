{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7786135",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'base.rl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7046/2708819124.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'base.rl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import gym\n",
    "import numpy\n",
    "from gym import wrappers\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import glob\n",
    "# from base.rl.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d591e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalPolicy(nn.Module):\n",
    "    def __init__(self, G=240, use_data_parallel=False, gpu_ids=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.G = G\n",
    "\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            Flatten(),  # (G*G, )\n",
    "            nn.Sigmoid(), # frontier_mask\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            Flatten(),\n",
    "            nn.Linear(self.G * self.G, 1),\n",
    "        )\n",
    "\n",
    "        if use_data_parallel:\n",
    "            self.actor = nn.DataParallel(\n",
    "                self.actor, device_ids=gpu_ids, output_device=gpu_ids[0],\n",
    "            )\n",
    "            self.critic = nn.DataParallel(\n",
    "                self.critic, device_ids=gpu_ids, output_device=gpu_ids[0],\n",
    "            )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _get_h12(self, inputs):\n",
    "        x = inputs[\"pose_in_map_at_t\"]\n",
    "        h = inputs[\"map_at_t\"]\n",
    "\n",
    "        h_1 = crop_map(h, x[:, :2], self.G)\n",
    "        h_2 = F.adaptive_max_pool2d(h, (self.G, self.G))\n",
    "\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def act(self, inputs, rnn_hxs, prev_actions, masks, deterministic=False):\n",
    "        \"\"\"\n",
    "        Note: inputs['pose_in_map_at_t'] must obey the following conventions:\n",
    "              origin at top-left, downward Y and rightward X in the map coordinate system.\n",
    "        \"\"\"\n",
    "        M = inputs[\"map_at_t\"].shape[2]\n",
    "        h_12 = self._get_h12(inputs)\n",
    "        '''\n",
    "        action_logits = self.actor(h_12)\n",
    "        dist = FixedCategorical(logits=action_logits)\n",
    "        value = self.critic(h_12)\n",
    "\n",
    "        if deterministic:\n",
    "            action = dist.mode()\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "        '''\n",
    "        #'''  # frontier_mask\n",
    "        action_logits = torch.clamp(self.actor(h_12), min=1e-4, max=1 - 1e-4)\n",
    "        value = self.critic(h_12)\n",
    "        action_mask = inputs[\"frontier_mask\"]\n",
    "        action_probs_mask = torch.where(action_mask == 1, action_logits, torch.ones_like(action_logits)*1e-7)\n",
    "        dist_2 = FixedCategorical(probs=action_probs_mask, validate_args=False)\n",
    "        if deterministic:\n",
    "            action = dist_2.mode()\n",
    "        else:\n",
    "            action = dist_2.sample()\n",
    "        action_log_probs = dist_2.log_probs(action)\n",
    "        #''' # frontier_mask\n",
    "        return value, action, action_log_probs, rnn_hxs\n",
    "\n",
    "    def get_value(self, inputs, rnn_hxs, prev_actions, masks):\n",
    "        h_12 = self._get_h12(inputs)\n",
    "        value = self.critic(h_12)\n",
    "        return value\n",
    "\n",
    "    def evaluate_actions(self, inputs, rnn_hxs, prev_actions, masks, action):\n",
    "        h_12 = self._get_h12(inputs)\n",
    "        '''\n",
    "        action_logits = self.actor(h_12)\n",
    "        dist = FixedCategorical(logits=action_logits)\n",
    "        value = self.critic(h_12)\n",
    "\n",
    "        action_log_probs = dist.log_probs(action)\n",
    "\n",
    "        dist_entropy = dist.entropy().mean()\n",
    "\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs\n",
    "        '''\n",
    "\n",
    "        #'''  # frontier_mask\n",
    "        action_logits = torch.clamp(self.actor(h_12), min=1e-4, max=1-1e-4)\n",
    "        value = self.critic(h_12)\n",
    "        action_mask = inputs[\"frontier_mask\"]\n",
    "        action_probs_mask = torch.where(action_mask == 1, action_logits, torch.ones_like(action_logits)*1e-7)\n",
    "        dist_2 = FixedCategorical(probs=action_probs_mask, validate_args=False)\n",
    "        action_log_probs = dist_2.log_probs(action)\n",
    "\n",
    "        dist_entropy = dist_2.entropy().mean()\n",
    "        return value, action_log_probs, dist_entropy, rnn_hxs\n",
    "        #'''  # frontier_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8273d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
