{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "06151c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from unet import UNet\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from utils.dataset import BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c36fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.4.0\n",
      "onnx.__version__: 1.10.1\n",
      "onnxruntime.__version__: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"onnx.__version__:\", onnx.__version__)\n",
    "print(\"onnxruntime.__version__:\", onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d2612ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: None\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnxpath)\n",
    "check = onnx.checker.check_model(onnx_model)\n",
    "print(\"check:\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6562ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "    bs, ch, H, W = h.size()\n",
    "    bs = torch.tensor(1)\n",
    "    ch = torch.tensor(4)\n",
    "    H = torch.tensor(481)\n",
    "    W = torch.tensor(481)\n",
    "    print(h.size())\n",
    "    \n",
    "    map_tmp = torch.zeros(1, 4, H+crop_size, W+crop_size)\n",
    "    map_c1 = torch.ones(H+crop_size, W+crop_size)\n",
    "    map_tmp[0][0] = map_c1\n",
    "   \n",
    "    for i in range(ch):\n",
    "        map_tmp[0][i][crop_size//2:crop_size//2+H, crop_size//2:crop_size//2+W] = h[0][i]\n",
    "    \n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    # print(map_tmp)\n",
    "    print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    \n",
    "    output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    \n",
    "    for i in range(ch):\n",
    "        if crop_size%2 == 0:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2, y_pos-crop_size//2:y_pos+crop_size//2]\n",
    "        else:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2+1, y_pos-crop_size//2:y_pos+crop_size//2+1]\n",
    "            \n",
    "    return output\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "\n",
    "        h_1 = crop_map(h, self.G)\n",
    "        print(h_1.size())\n",
    "        print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e87b8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[ 0, 20, 20]])\n",
      "tensor([[20, 20]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "print(torch.nonzero(inputs[0][3]==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1f700705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eva ONNX model and torch model with calc same value\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "eb762512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_Actor(\n",
       "  (actor): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(2, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (13): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_net = Global_Actor(240)\n",
    "rl_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "rl_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2ba7adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 481, 481])\n",
      "torch.Size([1, 4, 481, 481])\n",
      "x_pos:140, y_pos:140\n",
      "torch.Size([1, 4, 240, 240])\n",
      "tensor([[120, 120]])\n",
      "pth model output: tensor([[-0.0282, -0.0373, -0.0381,  ..., -0.0281, -0.0339, -0.0382]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([1, 57600])\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "print(x.size())\n",
    "torch_out = rl_net(x)\n",
    "print(\"pth model output:\", torch_out)\n",
    "print(torch_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ecf2dc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx model output: [array([[ 0.00421813,  0.0104795 ,  0.02085524, ..., -0.00355027,\n",
      "        -0.01217862, -0.01496383]], dtype=float32)]\n",
      "tor_out:  torch.Size([1, 57600])\n"
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnxpath)\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: x.cpu().numpy().astype(np.float32)})\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "\n",
    "print(\"onnx model output:\", ort_outs)\n",
    "print('tor_out: ', torch_out.shape)\n",
    "\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "# print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d4acd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
