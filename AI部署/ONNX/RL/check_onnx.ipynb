{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06151c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from unet import UNet\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from utils.dataset import BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3c36fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.4.0\n",
      "onnx.__version__: 1.10.1\n",
      "onnxruntime.__version__: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"onnx.__version__:\", onnx.__version__)\n",
    "print(\"onnxruntime.__version__:\", onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7d2612ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: None\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnxpath)\n",
    "check = onnx.checker.check_model(onnx_model)\n",
    "print(\"check:\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6562ebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, x, crop_size, mode=\"bilinear\"):\n",
    "    \"\"\"\n",
    "    Crops a tensor h centered around location x with size crop_size\n",
    "\n",
    "    Inputs:\n",
    "        h - (bs, F, H, W)\n",
    "        x - (bs, 2) --- (x, y) locations\n",
    "        crop_size - scalar integer\n",
    "\n",
    "    Conventions for x:\n",
    "        The origin is at the top-left, X is rightward, and Y is downward.\n",
    "    \"\"\"\n",
    "\n",
    "    bs, _, H, W = h.size()\n",
    "    Hby2 = (H - 1) / 2 if H % 2 == 1 else H // 2\n",
    "    Wby2 = (W - 1) / 2 if W % 2 == 1 else W // 2\n",
    "    start = -(crop_size - 1) / 2 if crop_size % 2 == 1 else -(crop_size // 2)\n",
    "    end = start + crop_size - 1\n",
    "    x_grid = (\n",
    "        torch.arange(start, end + 1, step=1)\n",
    "        .unsqueeze(0)\n",
    "        .expand(crop_size, -1)\n",
    "        .contiguous()\n",
    "        .float()\n",
    "    )\n",
    "    y_grid = (\n",
    "        torch.arange(start, end + 1, step=1)\n",
    "        .unsqueeze(1)\n",
    "        .expand(-1, crop_size)\n",
    "        .contiguous()\n",
    "        .float()\n",
    "    )\n",
    "    center_grid = torch.stack([x_grid, y_grid], dim=2).to(\n",
    "        h.device\n",
    "    )  # (crop_size, crop_size, 2)\n",
    "\n",
    "    x_pos = x[:, 0] - Wby2  # (bs, )\n",
    "    y_pos = x[:, 1] - Hby2  # (bs, )\n",
    "\n",
    "    crop_grid = center_grid.unsqueeze(0).expand(\n",
    "        bs, -1, -1, -1\n",
    "    )  # (bs, crop_size, crop_size, 2)\n",
    "    crop_grid = crop_grid.contiguous()\n",
    "\n",
    "    # Convert the grid to (-1, 1) range\n",
    "    crop_grid[:, :, :, 0] = (\n",
    "        crop_grid[:, :, :, 0] + x_pos.unsqueeze(1).unsqueeze(2)\n",
    "    ) / Wby2\n",
    "    crop_grid[:, :, :, 1] = (\n",
    "        crop_grid[:, :, :, 1] + y_pos.unsqueeze(1).unsqueeze(2)\n",
    "    ) / Hby2\n",
    "\n",
    "    h_cropped = F.grid_sample(h, crop_grid, mode=mode)\n",
    "\n",
    "    return h_cropped\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "\n",
    "        h_1 = crop_map(h, x[:, :2], self.G)\n",
    "        h_2 = F.adaptive_max_pool2d(h, (self.G, self.G))\n",
    "\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e87b8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[ 0, 20, 20]])\n",
      "tensor([[20, 20]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "print(torch.nonzero(inputs[0][3]==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f700705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eva ONNX model and torch model with calc same value\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eb762512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_Actor(\n",
       "  (actor): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(2, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (13): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_net = Global_Actor(240)\n",
    "rl_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "rl_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2ba7adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 481, 481])\n",
      "pth model output: tensor([[0.0042, 0.0042, 0.0042,  ..., 0.0042, 0.0042, 0.0042]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "print(x.size())\n",
    "torch_out = rl_net(x)\n",
    "print(\"pth model output:\", torch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ecf2dc2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\n index: 1 Got: 4 Expected: 8\n index: 2 Got: 480 Expected: 240\n index: 3 Got: 480 Expected: 240\n Please fix either the inputs or the model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32575/2816020078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# compute ONNX Runtime output prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mort_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mort_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# compare ONNX Runtime and PyTorch results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input for the following indices\n index: 1 Got: 4 Expected: 8\n index: 2 Got: 480 Expected: 240\n index: 3 Got: 480 Expected: 240\n Please fix either the inputs or the model."
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnxpath)\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name:to_numpy(x)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "\n",
    "print(\"onnx model output:\", ort_outs)\n",
    "print('tor_out: ', torch_out.shape)\n",
    "\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d90b668f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "print(a.size())\n",
    "a = torch.unsqueeze(a, 1)\n",
    "print(a.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
