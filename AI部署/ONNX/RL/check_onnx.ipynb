{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "06151c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "from unet import UNet\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from utils.dataset import BasicDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3c36fd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.__version__: 1.4.0\n",
      "onnx.__version__: 1.10.1\n",
      "onnxruntime.__version__: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(\"torch.__version__:\", torch.__version__)\n",
    "print(\"onnx.__version__:\", onnx.__version__)\n",
    "print(\"onnxruntime.__version__:\", onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "7d2612ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check: None\n"
     ]
    }
   ],
   "source": [
    "onnx_model = onnx.load(onnxpath)\n",
    "check = onnx.checker.check_model(onnx_model)\n",
    "print(\"check:\", check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e514287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "    bs = torch.tensor(1)\n",
    "    ch = torch.tensor(4)\n",
    "    H = torch.tensor(481)\n",
    "    W = torch.tensor(481)\n",
    "    \n",
    "    tmp_h_0 = torch.zeros(120, 481)\n",
    "    tmp_h_1 = torch.ones(120, 481)\n",
    "    tmp_v_0 = torch.zeros(721, 120)\n",
    "    tmp_v_1 = torch.ones(721, 120)\n",
    "    \n",
    "    ############### first map ######################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h0 = torch.cat((tmp_map[0][0], tmp_h_1), dim=0)\n",
    "    tmp_h0 = torch.cat((tmp_h_1, tmp_h0), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v0 = torch.cat((tmp_h0, tmp_v_1), dim=1)\n",
    "    tmp_v0 = torch.cat((tmp_v_1, tmp_v0), dim=1)\n",
    "    \n",
    "    ################ second map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h1 = torch.cat((tmp_map[0][1], tmp_h_0), dim=0)\n",
    "    tmp_h1 = torch.cat((tmp_h_0, tmp_h1), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v1 = torch.cat((tmp_h1, tmp_v_0), dim=1)\n",
    "    tmp_v1 = torch.cat((tmp_v_0, tmp_v1), dim=1)\n",
    "\n",
    "    ################ Third map ####################\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h2 = torch.cat((tmp_map[0][2], tmp_h_0), dim=0)\n",
    "    tmp_h2 = torch.cat((tmp_h_0, tmp_h2), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v2 = torch.cat((tmp_h2, tmp_v_0), dim=1)\n",
    "    tmp_v2 = torch.cat((tmp_v_0, tmp_v2), dim=1)\n",
    "    # print(\"tmp_v:\", tmp_v.size())\n",
    "\n",
    "    ################ Forth map ####################\n",
    "\n",
    "    # tensor_index = torch.linspace(120., 601., 481)\n",
    "    tmp_map = h\n",
    "    # print(\"h size::\", h.size())\n",
    "    \n",
    "    tmp_h3 = torch.cat((tmp_map[0][3], tmp_h_0), dim=0)\n",
    "    tmp_h3 = torch.cat((tmp_h_0, tmp_h3), dim=0)\n",
    "    # print(\"tmp_h:\", tmp_h.size())\n",
    "\n",
    "    tmp_v3 = torch.cat((tmp_h3, tmp_v_0), dim=1)\n",
    "    tmp_v3 = torch.cat((tmp_v_0, tmp_v3), dim=1)\n",
    "    \n",
    "    ################################################\n",
    "    map_tmp = torch.cat((torch.unsqueeze(tmp_v0, 0), torch.unsqueeze(tmp_v1, 0), \n",
    "                         torch.unsqueeze(tmp_v2, 0), torch.unsqueeze(tmp_v3, 0)), 0)\n",
    "    map_tmp = torch.unsqueeze(map_tmp, 0)\n",
    "    # print(\"after expanding map size:\", map_tmp.size())\n",
    "\n",
    "    ############ start crap map #####################\n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    \n",
    "    # print(map_tmp)\n",
    "    # print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    \n",
    "    \n",
    "    index_tensor = torch.tensor([-120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119])\n",
    "    index_tensor_x = index_tensor + x_pos\n",
    "    index_tensor_y = index_tensor + y_pos\n",
    "\n",
    "    tmp_crap_map_0 = torch.index_select(map_tmp[0][0], 1, index_tensor_y)\n",
    "    tmp_crap_map_0 = torch.index_select(tmp_crap_map_0, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_1 = torch.index_select(map_tmp[0][1], 1, index_tensor_y)\n",
    "    tmp_crap_map_1 = torch.index_select(tmp_crap_map_1, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_2 = torch.index_select(map_tmp[0][2], 1, index_tensor_y)\n",
    "    tmp_crap_map_2 = torch.index_select(tmp_crap_map_2, 0, index_tensor_x)\n",
    "    \n",
    "    tmp_crap_map_3 = torch.index_select(map_tmp[0][3], 1, index_tensor_y)\n",
    "    tmp_crap_map_3 = torch.index_select(tmp_crap_map_3, 0, index_tensor_x)\n",
    "    '''\n",
    "    output = torch.cat((torch.unsqueeze(tmp_crap_map_0, 0), torch.unsqueeze(tmp_crap_map_1, 0),\n",
    "                       torch.unsqueeze(tmp_crap_map_2, 0), torch.unsqueeze(tmp_crap_map_3, 0)), 0)\n",
    "    output = torch.unsqueeze(output, 0)\n",
    "    # print(\"ouput::::\",output.size())\n",
    "    '''\n",
    "    output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    '''\n",
    "    output[0][0] = tmp_crap_map_0\n",
    "    output[0][1] = tmp_crap_map_1\n",
    "    output[0][2] = tmp_crap_map_2\n",
    "    output[0][3] = tmp_crap_map_3\n",
    "    '''\n",
    "    tmp_0 = torch.unsqueeze(tmp_crap_map_0, 0)\n",
    "    tmp_1 = torch.unsqueeze(tmp_crap_map_1, 0)\n",
    "    tmp_2 = torch.unsqueeze(tmp_crap_map_2, 0)\n",
    "    tmp_3 = torch.unsqueeze(tmp_crap_map_3, 0)\n",
    "    \n",
    "    tmp = torch.cat((tmp_0, tmp_1), 0)\n",
    "    tmp = torch.cat((tmp, tmp_2), 0)\n",
    "    tmp = torch.cat((tmp, tmp_3), 0)\n",
    "    tmp = torch.unsqueeze(tmp, 0)\n",
    "  \n",
    "    output = tmp\n",
    "    # print(\"output:\", output.size())\n",
    "    \n",
    "    return output\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = torch.tensor(G)\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "        \n",
    "        h_1 = crop_map(h, self.G)\n",
    "        # print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "e87b8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[ 0, 20, 20]])\n",
      "tensor([[20, 20]])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "print(torch.nonzero(inputs[0][3]==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "1f700705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eva ONNX model and torch model with calc same value\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "eb762512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Global_Actor(\n",
       "  (actor): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(4, 4, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (7): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(4, 2, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (10): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(2, 1, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (13): Flatten()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_net = Global_Actor(240)\n",
    "rl_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "rl_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2ba7adcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 481, 481])\n",
      "pth model output: tensor([[-0.1337, -0.1574, -0.1537,  ..., -0.1086, -0.1252, -0.1249]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "torch.Size([1, 57600])\n"
     ]
    }
   ],
   "source": [
    "x = inputs\n",
    "print(x.size())\n",
    "torch_out = rl_net(x)\n",
    "print(\"pth model output:\", torch_out)\n",
    "print(torch_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "ecf2dc2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidGraph",
     "evalue": "[ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from test_conv_pool.onnx failed:This is an invalid model. Type Error: Type 'tensor(float)' of input parameter (88) of operator (Equal) in node () is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidGraph\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_59137/1700085329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mort_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnxruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monnxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# compute ONNX Runtime output prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mort_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mort_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_inference_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproviders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisabled_optimizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36m_create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0msession_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess_options\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInferenceSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_config_from_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidGraph\u001b[0m: [ONNXRuntimeError] : 10 : INVALID_GRAPH : Load model from test_conv_pool.onnx failed:This is an invalid model. Type Error: Type 'tensor(float)' of input parameter (88) of operator (Equal) in node () is invalid."
     ]
    }
   ],
   "source": [
    "ort_session = onnxruntime.InferenceSession(onnxpath)\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_outs = ort_session.run(None, {ort_session.get_inputs()[0].name: x.cpu().numpy().astype(np.float32)})\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "\n",
    "print(\"onnx model output:\", ort_outs)\n",
    "print('tor_out: ', torch_out.shape)\n",
    "\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "13d2c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "    bs, ch, H, W = h.size()\n",
    "    bs = torch.tensor(1)\n",
    "    ch = torch.tensor(4)\n",
    "    H = torch.tensor(481)\n",
    "    W = torch.tensor(481)\n",
    "    print(h.size())\n",
    "    \n",
    "    map_tmp = torch.zeros(1, 4, H+crop_size, W+crop_size)\n",
    "    map_c1 = torch.ones(H+crop_size, W+crop_size)\n",
    "    map_tmp[0][0] = map_c1\n",
    "   \n",
    "    for i in range(ch):\n",
    "        map_tmp[0][i][crop_size//2:crop_size//2+H, crop_size//2:crop_size//2+W] = h[0][i]\n",
    "    \n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    # print(map_tmp)\n",
    "    print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    \n",
    "    output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    \n",
    "    for i in range(ch):\n",
    "        if crop_size%2 == 0:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2, y_pos-crop_size//2:y_pos+crop_size//2]\n",
    "        else:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2+1, y_pos-crop_size//2:y_pos+crop_size//2+1]\n",
    "            \n",
    "    return output\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "\n",
    "        h_1 = crop_map(h, self.G)\n",
    "        print(h_1.size())\n",
    "        print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e931b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
