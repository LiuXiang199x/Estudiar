{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37166c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import gym\n",
    "import numpy\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from gym import wrappers\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import glob\n",
    "# from base.rl.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "76da0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "1.10.1\n",
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(torch.__version__)\n",
    "print(onnx.__version__)\n",
    "print(onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f1116726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "    bs, ch, H, W = h.size()\n",
    "    bs = torch.tensor(1)\n",
    "    ch = torch.tensor(4)\n",
    "    H = torch.tensor(481)\n",
    "    W = torch.tensor(481)\n",
    "    \n",
    "    map_tmp = torch.zeros(1, 3, H+crop_size, W+crop_size)\n",
    "    map_c1 = torch.ones(1, 1, H+crop_size, W+crop_size)\n",
    "    map_tmp = torch.cat((map_c1, map_tmp), dim=1)   # (1,4,721,721)\n",
    "    \n",
    "    for i in range(ch):\n",
    "        i = torch.tensor(i)\n",
    "        map_tmp[0][i][crop_size//2:crop_size//2+H, crop_size//2:crop_size//2+W] = h[0][i]\n",
    "        \n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    \n",
    "    # print(map_tmp)\n",
    "    print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    \n",
    "    output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    \n",
    "    for i in range(ch):\n",
    "        # if crop_size%2 == 0:\n",
    "        output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2, y_pos-crop_size//2:y_pos+crop_size//2]\n",
    "        # else:\n",
    "            # output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2+1, y_pos-crop_size//2:y_pos+crop_size//2+1]\n",
    "            \n",
    "    return output\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = torch.tensor(G)\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "        \n",
    "        h_1 = crop_map(h, self.G)\n",
    "        print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8156500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['0.weight', '0.bias', '1.weight', '1.bias', '1.running_mean', '1.running_var', '3.weight', '3.bias', '4.weight', '4.bias', '4.running_mean', '4.running_var', '6.weight', '6.bias', '7.weight', '7.bias', '7.running_mean', '7.running_var', '9.weight', '9.bias', '10.weight', '10.bias', '10.running_mean', '10.running_var', '12.weight', '12.bias'], unexpected_keys=['global_state_dict', 'extra_state'])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net = Global_Actor(240)\n",
    "actor_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "    # Initialize model with the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f015b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "inputs[0][3] = map3\n",
    "'''\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "yy = torch.nonzero(inputs[0][3]==1)\n",
    "print(type(yy))\n",
    "print(type(yy[:,:2]))\n",
    "print(yy == yy[:,:2])\n",
    "print(x.size())\n",
    "'''\n",
    "\n",
    "x = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9d066423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    M = 481\n",
    "    model = Global_Actor(240)\n",
    "    batch_size = 1\n",
    "    input_shape = (4, M, M)\n",
    "    \n",
    "    # Initialize model with the pretrained weights\n",
    "    # map_location = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    loaded_model = torch.load(pthfile, map_location=\"cpu\")\n",
    "    model.actor.load_state_dict(loaded_model, strict=False)\n",
    "    \n",
    "    # set the model to inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # data type nchw\n",
    "    # x = torch.rand(batch_size, *input_shape)\n",
    "    input_names = ['input']\n",
    "    output_names = ['output']\n",
    "    # # Export the model\n",
    "    torch.onnx.export(model,               # model being run\n",
    "                    x,                         # model input (or a tuple for multiple inputs)\n",
    "                    onnxpath,   # where to save the model (can be a file or file-like object)\n",
    "                    export_params=True,        # store the trained parameter weights inside the model file\n",
    "                    opset_version=10,          # the ONNX version to export the model to\n",
    "                    do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                    input_names = ['input'],   # the model's input names\n",
    "                    output_names = ['output'], # the model's output names\n",
    "                    dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                                  'output' : {0 : 'batch_size'}}\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4c1e9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_pos:140, y_pos:140\n",
      "tensor([[120, 120]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:72: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:4: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:5: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  \"\"\"\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:6: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  \n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:7: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  import sys\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:13: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  del sys.path[0]\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:14: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  \n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:28: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:30: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:30: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bba5ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.1109, -0.9178],\n",
      "          [-0.5196, -0.7122]],\n",
      "\n",
      "         [[-0.7214,  0.8115],\n",
      "          [-0.2302,  0.4412]],\n",
      "\n",
      "         [[-1.2322, -0.3478],\n",
      "          [-0.5297,  0.9830]]]])\n",
      "tensor([[[[ 0.4916, -1.1484],\n",
      "          [ 0.4830, -1.6397]]]])\n",
      "tensor([[[[-1.1109, -0.9178],\n",
      "          [-0.5196, -0.7122]],\n",
      "\n",
      "         [[-0.7214,  0.8115],\n",
      "          [-0.2302,  0.4412]],\n",
      "\n",
      "         [[-1.2322, -0.3478],\n",
      "          [-0.5297,  0.9830]],\n",
      "\n",
      "         [[ 0.4916, -1.1484],\n",
      "          [ 0.4830, -1.6397]]]])\n",
      "torch.Size([1, 4, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(1,3,2,2)\n",
    "b = torch.randn(1,1,2,2)\n",
    "print(a)\n",
    "print(b)\n",
    "c = torch.cat((a,b),dim=1)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c8dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "æ— ",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
