{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37166c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import gym\n",
    "import numpy\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from gym import wrappers\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional\n",
    "import glob\n",
    "# from base.rl.ppo import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76da0d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n",
      "1.10.1\n",
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pthfile = 'ckpt.test.pth'\n",
    "onnxpath = 'test_conv_pool.onnx'\n",
    "print(torch.__version__)\n",
    "print(onnx.__version__)\n",
    "print(onnxruntime.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1116726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_map(h, crop_size):\n",
    "    \n",
    "    bs, ch, H, W = h.size()\n",
    "    print(h.size())\n",
    "    \n",
    "    map_tmp = torch.zeros(1, 4, H+crop_size, W+crop_size)\n",
    "    map_c1 = torch.ones(H+crop_size, W+crop_size)\n",
    "    map_tmp[0][0] = map_c1\n",
    "   \n",
    "    for i in range(ch):\n",
    "        map_tmp[0][i][crop_size//2:crop_size//2+H, crop_size//2:crop_size//2+W] = h[0][i]\n",
    "    \n",
    "    x_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,0][0]\n",
    "    y_pos = torch.nonzero(map_tmp[0][3]==1)[:][:,1][0]\n",
    "    \n",
    "    # print(map_tmp)\n",
    "    print(\"x_pos:{}, y_pos:{}\".format(x_pos,y_pos))\n",
    "    # print(map_tmp.size())\n",
    "    # print(torch.nonzero(map_tmp[0][3]==1))\n",
    "    \n",
    "    output = torch.randn(1, 4, crop_size, crop_size)\n",
    "    \n",
    "    for i in range(ch):\n",
    "        if crop_size%2 == 0:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2, y_pos-crop_size//2:y_pos+crop_size//2]\n",
    "        else:\n",
    "            output[0][i] = map_tmp[0][i][x_pos-crop_size//2:x_pos+crop_size//2+1, y_pos-crop_size//2:y_pos+crop_size//2+1]\n",
    "            \n",
    "    return output\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "class Global_Actor(nn.Module):\n",
    "    def __init__(self, G):\n",
    "        super().__init__()\n",
    "        self.G = G\n",
    "        self.actor = nn.Sequential(  # (8, G, G)\n",
    "            nn.Conv2d(8, 8, 3, padding=1),  # (8, G, G)\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 4, 3, padding=1),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 4, 5, padding=2),  # (4, G, G)\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(4, 2, 5, padding=2),  # (2, G, G)\n",
    "            nn.BatchNorm2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(2, 1, 5, padding=2),  # (1, G, G)\n",
    "            # nn.BatchNorm2d(1),\n",
    "            Flatten(),  # (G*G, )\n",
    "            # nn.Sigmoid(),  # added for non-negative\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _get_h12(self, inputs): # inputs needs to be a tensor, i.e., original inputs[\"map_at_t\"], (bs, 4, M, M), channel 3 means one-hot pose, channel 0~1 means global map\n",
    "        # x = inputs[\"pose_in_map_at_t\"]  # (bs,2)\n",
    "        # map_at_t (4, m, m)\n",
    "        # x = torch.nonzero(inputs[0][3]==1)\n",
    "        h = inputs\n",
    "\n",
    "        h_1 = crop_map(h, self.G)\n",
    "        print(h_1.size())\n",
    "        print(torch.nonzero(h_1[0][3]==1))\n",
    "        h_2 = F.max_pool2d(h, (2, 2))\n",
    "# adaptive_max_pool2d\n",
    "        h_12 = torch.cat([h_1, h_2], dim=1)\n",
    "\n",
    "        return h_12\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x1 = self._get_h12(inputs)\n",
    "        x2 = self.actor(x1)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8156500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['0.weight', '0.bias', '1.weight', '1.bias', '1.running_mean', '1.running_var', '3.weight', '3.bias', '4.weight', '4.bias', '4.running_mean', '4.running_var', '6.weight', '6.bias', '7.weight', '7.bias', '7.running_mean', '7.running_var', '9.weight', '9.bias', '10.weight', '10.bias', '10.running_mean', '10.running_var', '12.weight', '12.bias'], unexpected_keys=['module.0.weight', 'module.0.bias', 'module.1.weight', 'module.1.bias', 'module.1.running_mean', 'module.1.running_var', 'module.1.num_batches_tracked', 'module.3.weight', 'module.3.bias', 'module.4.weight', 'module.4.bias', 'module.4.running_mean', 'module.4.running_var', 'module.4.num_batches_tracked', 'module.6.weight', 'module.6.bias', 'module.7.weight', 'module.7.bias', 'module.7.running_mean', 'module.7.running_var', 'module.7.num_batches_tracked', 'module.9.weight', 'module.9.bias', 'module.10.weight', 'module.10.bias', 'module.10.running_mean', 'module.10.running_var', 'module.10.num_batches_tracked', 'module.12.weight', 'module.12.bias'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_net = Global_Actor(240)\n",
    "actor_net.actor.load_state_dict(torch.load(pthfile, map_location='cpu'),strict=False)\n",
    "    # Initialize model with the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f015b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "tensor([[ 0, 20, 20]])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[True, True]])\n",
      "torch.Size([1, 4, 481, 481])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1, 4, 481, 481)\n",
    "\n",
    "map3 = torch.zeros(1, 481, 481)\n",
    "map3[0][20][20] = 1.\n",
    "print(map3)\n",
    "print(torch.nonzero(map3==1))\n",
    "inputs[0][3] = map3\n",
    "yy = torch.nonzero(inputs[0][3]==1)\n",
    "print(type(yy))\n",
    "print(type(yy[:,:2]))\n",
    "print(yy == yy[:,:2])\n",
    "\n",
    "x = inputs\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d066423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    M = 481\n",
    "    model = Global_Actor(240)\n",
    "    batch_size = 1\n",
    "    input_shape = (4, M, M)\n",
    "    \n",
    "    # Initialize model with the pretrained weights\n",
    "    map_location = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "    loaded_model = torch.load(pthfile, map_location=map_location)\n",
    "    model.actor.load_state_dict(loaded_model, strict=False)\n",
    "    \n",
    "    # set the model to inference mode\n",
    "    model.eval()\n",
    "    \n",
    "    # data type nchw\n",
    "    # x = torch.rand(batch_size, *input_shape)\n",
    "    \n",
    "    torch.onnx.export(model,               # model being run\n",
    "                x,                         # model input (or a tuple for multiple inputs)\n",
    "                onnxpath,   # where to save the model (can be a file or file-like object)\n",
    "                export_params=True,        # store the trained parameter weights inside the model file\n",
    "                opset_version=10,          # the ONNX version to export the model to\n",
    "                do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                input_names = ['input'],   # the model's input names\n",
    "                output_names = ['output'], # the model's output names\n",
    "                dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes\n",
    "                              'output' : {0 : 'batch_size'}}\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c1e9656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 481, 481])\n",
      "x_pos:140, y_pos:140\n",
      "torch.Size([1, 4, 240, 240])\n",
      "tensor([[120, 120]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:4: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  after removing the cwd from sys.path.\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:8: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n",
      "  \n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:10: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:11: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:11: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:23: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:25: TracerWarning: Converting a tensor to a Python index might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:25: TracerWarning: There are 3 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n",
      "/home/xiang/anaconda3/envs/py37/lib/python3.7/site-packages/ipykernel_launcher.py:69: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3532621f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "tensor([[0.3203, 0.6363]])\n",
      "tensor([[1., 1.]])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((1,2))\n",
    "print(a.size())\n",
    "print(a)\n",
    "b = torch.ones(1,2)\n",
    "print(b)\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26a51963",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.randn(4,6,6)\n",
    "# print(c)\n",
    "# print(c[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da5ab889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [1, 1]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([[1,2,3],[4,2,6]])\n",
    "target = 4\n",
    "a = torch.nonzero(data==2)\n",
    "print(a)\n",
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eb37ef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 23\n",
      "(1, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type() takes 1 or 3 arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31385/2999991581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# a = *(1,2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type() takes 1 or 3 arguments"
     ]
    }
   ],
   "source": [
    "print(*(1,23))\n",
    "print((1,2))\n",
    "print(type(*(1,3)))\n",
    "# a = *(1,2)\n",
    "print(a)\n",
    "# print(type(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "650c67ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6939, -0.3168, -0.5826],\n",
      "        [ 0.4609,  1.3670, -0.4170]])\n",
      "tensor([[ 1.6939e+00, -3.1676e-01, -5.8264e-01, -1.2504e+00,  4.0678e-01,\n",
      "          9.8056e-02],\n",
      "        [ 4.6094e-01,  1.3670e+00, -4.1705e-01, -1.9146e+00, -1.3156e-03,\n",
      "          4.9554e-01]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "print(a)\n",
    "b = torch.randn(2,3)\n",
    "print(torch.cat((a,b),dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc0256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
